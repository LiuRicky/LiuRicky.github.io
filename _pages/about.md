---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

I’m a second-year graduate student at the School of Information, Renmin University of China. Before that, I received my Bachelor’s degree in 2021 from South China University of Technology. Right now I focus on  <span style="color: red;">I'm looking for Ph.D. student positions for 2024 Fall.</span>


My research interest includes computer vision and multi-modal learning. I have published 2 papers at the top international AI conferences. Here is my google scholar <a href='https://scholar.google.com/citations?user=X-OlO2gAAAAJ&hl=en'>page</a>.


# 🔥 News
- *2022.11*: &nbsp;🎉🎉 One paper is accepted by AAAI 2023! 
- *2022.10*: &nbsp;🎉🎉 Our team rank the 1st in Trecvid 2022 VTT task! 
- *2022.05*: &nbsp;🎉🎉 One paper is accepted by ECCV 2023!  

# 📝 Publications 

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">AAAI 2023</div><img src='images/TOKEN_MIX.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Token Mixing: Parameter-Efficient Transfer Learning from Image-Language to Video-Language]()

**Yuqi Liu**, Luhui Xu, Pengfei Xiong, Qin Jin

[**Project**](https://github.com/yuqi657/video_language_model) 
- We study how to transfer knowledge from image-language model to video-language tasks. 
- We have implemented several components proposed by recent works.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ECCV 2022</div><img src='images/TS2_NET.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[TS2-Net: Token Shift and Selection Transformer for Text-Video Retrieval](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136740311.pdf) [**Project**](https://github.com/yuqi657/ts2_net) 

**Yuqi Liu**, Pengfei Xiong, Luhui Xu, Shengming Cao, Qin Jin

- TS2-Net is a text-video retrieval model based on [CLIP](https://github.com/openai/CLIP). 
- We propose our token shift transformer and token selection transformer.
</div>
</div>

- [TRECVID 2022 task: Video to Text Description
](https://www-nlpir.nist.gov/projects/tvpubs/tv22.papers/rucaim3-tencent.pdf) [**Project**](https://github.com/yuezih/BLIP4video)

Zihao Yue, Yuqi Liu, Liang Zhang, Linli Yao, Qin Jin **Trecvid VTT 2022**

# 🎖 Honors and Awards
- *2019.11* National Scholarship.
- *2018.11* National Scholarship.

# 📖 Educations
- *2021.09 - 2024.06 (now)*, M.Phil., School of Information, Remin University of China. 
- *2017.09 - 2021.06*, B.E., School of Software Engineering, South China University of Technology.

<!-- # 💬 Invited Talks
- *2021.06*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2021.03*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.  \| [\[video\]](https://github.com/) -->

# 💻 Internships
- *2022.01 - 2022.10*, Group of video computing, Tencent, China.
- *2020.04 - 2020.10*, Group of wechatPay HK, Tencent, China.